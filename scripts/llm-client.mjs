#!/usr/bin/env node
/**
 * LLM å®¢æˆ·ç«¯ - ç”¨äº Continuous Learning çš„æ¨¡å‹è°ƒç”¨
 * æ”¯æŒå¤šç§ providerï¼šopenclaw, openai, anthropic
 */

import { readFile } from 'fs/promises';
import { join } from 'path';
import { homedir } from 'os';

const CONFIG_PATH = join(homedir(), '.config', 'continuous-learning', 'config.json');

async function loadConfig() {
  try {
    const content = await readFile(CONFIG_PATH, 'utf-8');
    return JSON.parse(content);
  } catch {
    return { llm: { provider: 'openclaw' } };
  }
}

/**
 * è°ƒç”¨ LLM åˆ†æå¯¹è¯å†…å®¹
 */
export async function analyzeConversation(messages, config = null) {
  if (!config) {
    config = await loadConfig();
  }
  
  const provider = config.llm?.provider || 'openclaw';
  
  switch (provider) {
    case 'openclaw':
      return await analyzeWithOpenClaw(messages, config);
    case 'openai':
      return await analyzeWithOpenAI(messages, config);
    case 'anthropic':
      return await analyzeWithAnthropic(messages, config);
    default:
      throw new Error(`Unknown LLM provider: ${provider}`);
  }
}

/**
 * ä½¿ç”¨ OpenClaw å†…éƒ¨æœºåˆ¶åˆ†æ
 * è¿™é‡Œé€šè¿‡å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åè°ƒç”¨ç³»ç»Ÿå‘½ä»¤
 */
async function analyzeWithOpenClaw(messages, config) {
  // æ„å»ºåˆ†ææç¤º
  const prompt = buildAnalysisPrompt(messages);
  
  // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨ OpenClaw çš„ API
  // æˆ–è€…é€šè¿‡æŸç§ IPC æœºåˆ¶ä¸ä¸»è¿›ç¨‹é€šä¿¡
  
  // ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›æ¨¡æ‹Ÿç»“æœ
  // TODO: å®ç°çœŸæ­£çš„ OpenClaw é›†æˆ
  
  console.log('ğŸ¤– Using OpenClaw provider (simulated)');
  
  // æ¨¡æ‹Ÿåˆ†æç»“æœ
  return simulateAnalysis(messages);
}

/**
 * ä½¿ç”¨ OpenAI API åˆ†æ
 */
async function analyzeWithOpenAI(messages, config) {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY not set');
  }
  
  const prompt = buildAnalysisPrompt(messages);
  
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: config.llm?.model || 'gpt-4o-mini',
      messages: [
        { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»å¯¹è¯ä¸­æå–å…³é”®ä¿¡æ¯ã€‚' },
        { role: 'user', content: prompt }
      ],
      temperature: config.llm?.temperature || 0.3,
      max_tokens: config.llm?.max_tokens_per_analysis || 2000,
      response_format: { type: 'json_object' }
    })
  });
  
  if (!response.ok) {
    throw new Error(`OpenAI API error: ${response.status}`);
  }
  
  const data = await response.json();
  const content = data.choices[0]?.message?.content;
  
  return JSON.parse(content);
}

/**
 * ä½¿ç”¨ Anthropic API åˆ†æ
 */
async function analyzeWithAnthropic(messages, config) {
  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey) {
    throw new Error('ANTHROPIC_API_KEY not set');
  }
  
  const prompt = buildAnalysisPrompt(messages);
  
  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01',
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: config.llm?.model || 'claude-3-haiku-20240307',
      max_tokens: config.llm?.max_tokens_per_analysis || 2000,
      temperature: config.llm?.temperature || 0.3,
      messages: [{ role: 'user', content: prompt }]
    })
  });
  
  if (!response.ok) {
    throw new Error(`Anthropic API error: ${response.status}`);
  }
  
  const data = await response.json();
  const content = data.content[0]?.text;
  
  // æå– JSON éƒ¨åˆ†
  const jsonMatch = content.match(/\{[\s\S]*\}/);
  if (jsonMatch) {
    return JSON.parse(jsonMatch[0]);
  }
  
  throw new Error('Could not parse LLM response as JSON');
}

/**
 * æ„å»ºåˆ†ææç¤º
 */
function buildAnalysisPrompt(messages) {
  const conversationText = messages
    .map(m => `[${m.role === 'user' ? 'ç”¨æˆ·' : 'åŠ©æ‰‹'}] ${m.content.slice(0, 800)}${m.content.length > 800 ? '...' : ''}`)
    .join('\n\n');
  
  return `è¯·åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œæå–å…³äºç”¨æˆ·çš„å…³é”®å­¦ä¹ ç‚¹ã€‚è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚

å¯¹è¯å†…å®¹ï¼š
${conversationText}

è¯·æå–ä»¥ä¸‹å†…å®¹å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºï¼ˆåªè¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ï¼‰ï¼š
{
  "facts": ["äº‹å®1", "äº‹å®2"],
  "preferences": ["åå¥½1", "åå¥½2"],
  "decisions": ["å†³ç­–1"],
  "commitments": ["æ‰¿è¯º1"],
  "insights": ["æ´å¯Ÿ1"],
  "questions": ["é—®é¢˜1"]
}

æå–è§„åˆ™ï¼š
- facts: ç”¨æˆ·æ˜ç¡®é™ˆè¿°çš„äº‹å®ä¿¡æ¯ï¼ˆå¦‚"æˆ‘åœ¨ä¸Šæµ·å·¥ä½œ"ï¼‰
- preferences: ç”¨æˆ·çš„å–œå¥½å’Œåå¥½ï¼ˆå¦‚"æˆ‘å–œæ¬¢è¡¨æ ¼å±•ç¤º"ï¼‰
- decisions: ç”¨æˆ·åšå‡ºçš„å†³ç­–æˆ–é€‰æ‹©
- commitments: ç”¨æˆ·æ‰¿è¯ºè¦åšçš„äº‹æƒ…æˆ–è·Ÿè¿›äº‹é¡¹
- insights: å¯¹ç”¨æˆ·è¡Œä¸ºæ¨¡å¼çš„æ·±å±‚æ´å¯Ÿ
- questions: ç”¨æˆ·è¡¨è¾¾çš„ç–‘é—®æˆ–å…´è¶£ç‚¹

è¯·ç”¨ä¸­æ–‡è¾“å‡ºï¼Œå¦‚æœæ²¡æœ‰æŸç±»å†…å®¹ï¼Œè¿”å›ç©ºæ•°ç»„ã€‚`;
}

/**
 * æ¨¡æ‹Ÿåˆ†æï¼ˆç”¨äºæµ‹è¯•ï¼‰
 */
function simulateAnalysis(messages) {
  // ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼Œå®é™…åº”è¯¥ç”¨ LLM
  const text = messages.map(m => m.content).join(' ').toLowerCase();
  
  const result = {
    facts: [],
    preferences: [],
    decisions: [],
    commitments: [],
    insights: [],
    questions: []
  };
  
  // ç®€å•çš„æ¨¡å¼åŒ¹é…
  if (text.includes('å–œæ¬¢') || text.includes('åå¥½')) {
    const match = text.match(/å–œæ¬¢(.{2,20}?)[ï¼Œã€‚ï¼›]/);
    if (match) result.preferences.push(`å–œæ¬¢${match[1]}`);
  }
  
  if (text.includes('å†³å®š') || text.includes('é€‰æ‹©')) {
    const match = text.match(/å†³å®š(.{2,20}?)[ï¼Œã€‚ï¼›]/);
    if (match) result.decisions.push(`å†³å®š${match[1]}`);
  }
  
  if (text.includes('æ˜å¤©') || text.includes('ä¸‹æ¬¡') || text.includes('ç¨å')) {
    const match = text.match(/(æ˜å¤©.+?)[ï¼Œã€‚ï¼›]/);
    if (match) result.commitments.push(match[1]);
  }
  
  // å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹ï¼Œæ·»åŠ ä¸€ä¸ªæç¤º
  if (Object.values(result).every(arr => arr.length === 0)) {
    result.insights.push('å¯¹è¯å†…å®¹è¾ƒä¸ºç®€çŸ­ï¼Œæœªæå–åˆ°æ˜ç¡®çš„å¯å­¦ä¹ ä¿¡æ¯');
  }
  
  return result;
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
if (import.meta.url === `file://${process.argv[1]}`) {
  console.log('LLM Client for Continuous Learning');
  console.log('Usage: import { analyzeConversation } from "./llm-client.mjs"');
}
